{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from os.path import splitext\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from PIL import Image\n",
    "import os\n",
    "from os.path import splitext\n",
    "from os import listdir\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "from skimage.metrics import peak_signal_noise_ratio, normalized_root_mse, structural_similarity\n",
    "\n",
    "import pytorch_ssim\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicDataset(Dataset):\n",
    "\n",
    "    def __init__(self, imgs_dir, masks_dir,  filenames, train):\n",
    "        self.imgs_dir = imgs_dir\n",
    "        self.masks_dir = masks_dir\n",
    "        self.train = train\n",
    "\n",
    "        if(self.train):\n",
    "            self.transforms = train_transforms\n",
    "        else:\n",
    "            self.transforms = test_transforms\n",
    "        \n",
    "        self.ids = filenames\n",
    "        self.gt = np.loadtxt(masks_dir + '/test.txt', delimiter=', ')\n",
    "        self.gt = torch.from_numpy(self.gt)\n",
    "        self.masks = self.gt[:, :7]\n",
    "        self.cameras = self.gt[:, 7:9]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.ids[i]\n",
    "        img_file = glob(self.imgs_dir + idx + '.png')\n",
    "        mask_id = int(idx.split('_')[1])-1\n",
    "        mask = self.masks[mask_id, :]\n",
    "\n",
    "        #assert len(img_file) == 1,             f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
    "        #assert len(mask) == 10,             f'Either no mask or multiple masks found for the ID {idx}: {mask}'\n",
    "        \n",
    "        img = Image.open(img_file[0])\n",
    "        \n",
    "        if(img.mode == 'P'):\n",
    "            img = img.convert('RGB')\n",
    "            img.save(self.imgs_dir + idx + '.png')\n",
    "\n",
    "        return {'image': (self.transforms(img)).float(), 'mask': (mask).float(), 'label': mask_id, 'camera' : self.cameras[mask_id]}\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "\n",
    "    def __init__(self, gt_path, pred_path):\n",
    "        self.gt_path = gt_path\n",
    "        self.pred_path = pred_path\n",
    "\n",
    "        self.gt_filenames = glob(self.gt_path + 'gt/*.png')\n",
    "        self.pred_filenames = glob(self.pred_path + 'pred/*.png')\n",
    "\n",
    "        #assert len(self.gt_filenames) == self.pred_filenames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pred_filenames)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        gt_image = self.gt_path + 'gt/' + os.path.basename(self.pred_filenames[i])\n",
    "        pred_image = self.pred_path + 'pred/' + os.path.basename(self.pred_filenames[i])\n",
    "\n",
    "        img1 = Image.open(gt_image).convert('RGB')\n",
    "        img2 = Image.open(pred_image).convert('RGB')\n",
    "\n",
    "        return {'gt': transforms.ToTensor()(img1), 'pred': transforms.ToTensor()(img2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([\n",
    "                           #transforms.Resize(90),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                       ])\n",
    "\n",
    "def loadModel(path, filename, device):\n",
    "    checkpoint = torch.load(path+'/'+filename, map_location=device)\n",
    "    epoch = checkpoint['epoch']\n",
    "    model = checkpoint['model']\n",
    "    optimizer = checkpoint['optimizer']\n",
    "    lr_sched = checkpoint['lr_sched']\n",
    "\n",
    "    return epoch, model, optimizer, lr_sched\n",
    "\n",
    "def getX(radius, theta, phi):\n",
    "    return radius * math.sin(phi*0.01745) * math.cos(theta*0.01745);\n",
    "\n",
    "\n",
    "def getY(radius, theta, phi):\n",
    "    return radius * math.sin(phi*0.01745) * math.sin(theta*0.01745);\n",
    "\n",
    "\n",
    "def getZ(radius, theta, phi):\n",
    "    return radius * math.cos(phi*0.01745);\n",
    "\n",
    "def getLoss(pred, target):\n",
    "    loss_Kd = ( (target[:, 0:3] - pred[:, 0:3] )**2).sum()\n",
    "    factor = torch.unsqueeze((1-torch.pow(target[:,6], 1/3)),dim=1)\n",
    "    loss_Ks = ( (target[:, 3:6] - pred[:, 3:6] )**2 * factor ).sum()\n",
    "    loss_r = ( (target[:, 6] - pred[:, 6] )**2).sum()\n",
    "    return loss_Kd + loss_Ks + (loss_r*3)\n",
    "\n",
    "def test(model, device, iterator):\n",
    "    epoch_loss = 0\n",
    "    global_step = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    prediction_list = []\n",
    "    label_list = []\n",
    "    camera_list = []\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "\n",
    "            x = batch['image']\n",
    "            y = batch['mask']\n",
    "            labels =  batch['label']\n",
    "            cameras = batch['camera']\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            fx = model(x)\n",
    "\n",
    "            prediction_list.append(torch.squeeze(fx).cpu())\n",
    "            label_list.append(torch.squeeze(labels))\n",
    "            camera_list.append(torch.squeeze(cameras))\n",
    "            loss = getLoss(fx, y)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "    return epoch_loss / len(iterator), ((torch.stack(prediction_list)).view(-1,7)).numpy(), np.squeeze(((torch.stack(label_list)).view(-1,1)).numpy()), np.squeeze(((torch.stack(camera_list)).view(-1,2)).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################################################################################\n",
    "\n",
    "dir_img = 'maps-gt/'\n",
    "dir_mask = 'masks/'\n",
    "architecture = 'resnext101_32x8d'\n",
    "model_folder = './'\n",
    "save_model_folder = model_folder + ''\n",
    "checkpoint_file = 'efficient_44.pth'\n",
    "in_feat, out_feat = 2048, 7\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "batch_size = 300\n",
    "test_file_names = np.array([splitext(file)[0] for file in listdir(dir_img) if not file.startswith('.')])\n",
    "test_dataset = BasicDataset(dir_img, dir_mask, test_file_names, train=False)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True, num_workers=32, pin_memory=True)\n",
    "#torch.save(test_iterator, './test_loader.pth')\n",
    "\n",
    "device = torch.device('cuda')\n",
    "# model = torch.hub.load('pytorch/vision:v0.5.0', architecture, pretrained=False)\n",
    "# model.fc = nn.Sequential(\n",
    "#             nn.Linear(in_features=in_feat, out_features=out_feat, bias=True),\n",
    "#             nn.Sigmoid()\n",
    "#             )\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "model = EfficientNet.from_name('efficientnet-b0')\n",
    "model._fc = nn.Sequential(\n",
    "            nn.Linear(in_features=1280, out_features=7, bias=True),\n",
    "            nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "model.to(device)\n",
    "epoch, saved_model, optimizer, lr_sched = loadModel(save_model_folder, checkpoint_file, device)\n",
    "model.load_state_dict(saved_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 : Predicting properties from test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, entire_predict, mat_labels, camera_list = test(model, device, test_iterator)\n",
    "result = np.concatenate((entire_predict, np.expand_dims(mat_labels,axis=1), camera_list), axis=1)\n",
    "np.savetxt(save_model_folder+'predictions_with_ids.txt', result, fmt='%.3f', delimiter =', ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Using predicted properties to re-render images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d6e1487be643c9a2fa4f0c4bbc33ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################################################################\n",
    "\n",
    "prediction_set = np.loadtxt(save_model_folder+'predictions_with_ids.txt', delimiter=', ')\n",
    "GT_PATH = 'masks/'\n",
    "ground_truth =  np.loadtxt(GT_PATH+'test.txt', delimiter=', ')[:,:9]\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "for counter in tqdm(range(len(prediction_set))):\n",
    "    \n",
    "    materials = prediction_set[counter][:7]\n",
    "    label = int(prediction_set[counter][7])\n",
    "    camera = ground_truth[label][7:9]\n",
    "    mf = open(\"utils/materials.pbrt\", \"w+\")\n",
    "    mf.write('MakeNamedMaterial \\\"custommaterial\\\" \\\"string type\\\" \\\"uber\\\"\\n' + '\\n\\\"rgb Kd\\\" [' + str(materials[0]) + ' ' + str(materials[1]) + ' ' + str(materials[2]) + ']'+ '\\n\\\"rgb Ks\\\" [' + str(materials[3]) + ' ' + str(materials[4]) + ' ' + str(materials[5]) + '] ' + '\\n\\\"float roughness\\\" [' + str(materials[6]) + ']\\n')\n",
    "    mf.close()\n",
    "\n",
    "    directory = model_folder+ 'efficient-pred/'\n",
    "\n",
    "    cam_pos = getX(2, camera[1], camera[0]), getY(2, camera[1], camera[0]), getZ(2, camera[1], camera[0])\n",
    "\n",
    "    cf = open(\"utils/camera.pbrt\", \"w+\")\n",
    "    ff = open(\"utils/film.pbrt\", \"w+\")\n",
    "    cf.write(\"LookAt \")\n",
    "    cf.write(\"\\n%f %f %f\\n0 0 0\\n0 1 0\" % (cam_pos[0], cam_pos[2], cam_pos[1]))\n",
    "    ff.write(\"Film \\\"image\\\" \\\"string filename\\\" \\\"%s_%s_%s.png\\\" \\\"integer xresolution\\\" [128] \\\"integer yresolution\\\" [128]\" %  (str(directory) +\"/\"+ str(label+1), str(int(camera[0])), str(int(camera[1])) ))\n",
    "    cf.close()\n",
    "    ff.close()\n",
    "    os.system(r\"/home/farhan/Masters_Thesis/pbrt-v3/cmake-build-release/pbrt --quiet --nthreads=64 utils/scene.pbrt\")\n",
    "    #os.rename(\"/home/farhan/Farhan_Thesis_Codes/pbrt_experiments/kitchen/materials_props.txt\", \"materials_phi_%s_theta_%s.txt\" %  (str(cam_pos[3]), str(cam_pos[4])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda')\n",
    "# sum_ssim = 0\n",
    "# sum_mse = 0\n",
    "# sum_psnr = 0\n",
    "\n",
    "# gt_dir = '/home/farhan/Farhan_Thesis_Codes/pbrt_experiments/test-images/'\n",
    "# pred_dir = '/home/farhan/Farhan_Thesis_Codes/new_codes/resnet18-cedar/'\n",
    "# t_data = TestDataset(gt_dir, pred_dir)\n",
    "# train_iterator = DataLoader(t_data, batch_size=16, shuffle=True, num_workers=32, pin_memory=True)\n",
    "\n",
    "# for batch in train_iterator:\n",
    "#     gt_img = batch['gt']\n",
    "#     target_img = batch['pred']\n",
    "\n",
    "#     gt_img = gt_img.to(device)\n",
    "#     target_img = target_img.to(device)\n",
    "    \n",
    "#     sum_ssim += pytorch_ssim.ssim(gt_img, target_img)\n",
    "#     sum_mse += ((gt_img-target_img)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9d92e341b44f4f8ed27e23bae53ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gt_dir = 'new-gt/'\n",
    "pred_dir = './'\n",
    "\n",
    "sum_psnr = 0\n",
    "sum_ssim = 0\n",
    "sum_nrmse = 0\n",
    "\n",
    "pred_images = glob(pred_dir+'efficient-pred/*.png')\n",
    "\n",
    "for i in tqdm(range(len(pred_images))):\n",
    "    file = pred_images[i]\n",
    "    base_name = os.path.basename(file)\n",
    "    img1 = np.array(Image.open(gt_dir + base_name).convert('RGB'))\n",
    "    img2 = np.array(Image.open(pred_dir+'efficient-pred/'+base_name).convert('RGB'))\n",
    "    sum_psnr += peak_signal_noise_ratio(img1, img2)\n",
    "    sum_ssim += structural_similarity(img1, img2, multichannel=True)\n",
    "    sum_nrmse += normalized_root_mse(img1, img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45.490021732690025, 0.03538263390718832, 0.003910770852089129)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_psnr/30000, sum_nrmse/30000, 1/(sum_ssim/30000)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46.36415205005039, 0.034913718536975016, 0.004013628354597731)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_psnr/30000, sum_nrmse/30000, 1/(sum_ssim/30000)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47.255119025251325, 0.03253375369412408, 0.003977283175405244)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_psnr/30000, sum_nrmse/30000, 1/(sum_ssim/30000)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNext50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47.43280293066334, 0.030880006816994387, 0.0037543722624024944)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_psnr/30000, sum_nrmse/30000, 1/(sum_ssim/30000)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNext101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(47.583155822177254, 0.030717176250682005, 0.003572391948516218)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_psnr/30000, sum_nrmse/30000, 1/(sum_ssim/30000)-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41.6840961704434,\n",
       " 0.05638451460035347,\n",
       " 0.9932812720944176,\n",
       " 0.00676417455391598)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_psnr/30000, sum_nrmse/30000, sum_ssim/30000, 1/(sum_ssim/30000) -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WideResnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48.052561074743124,\n",
       " 0.0287378758416173,\n",
       " 0.996754223762731,\n",
       " 0.0032563456064587104)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_psnr/30000, sum_nrmse/30000, sum_ssim/30000, 1/(sum_ssim/30000) -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EfficientNet-B0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30.710896729251537, 0.18382442893359596, 0.029410542354880276)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_psnr/30000, sum_nrmse/30000, 1/(sum_ssim/30000)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
